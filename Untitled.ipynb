{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "print(sys.path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "from aggregator.attention import AttentionLoop\n",
    "from torch.utils.data import Dataset, DataLoader,ConcatDataset\n",
    "\n",
    "\n",
    "\n",
    "def getTensorData(path_to_folder,idx):\n",
    "\n",
    "    data = torch.load(f'{path_to_folder}/pca_{idx}.pt')\n",
    "    label = torch.load(f'{path_to_folder}/label.pt') \n",
    "\n",
    "    perm = torch.randperm(len(label))\n",
    "    data = data[:,perm]\n",
    "    label = label[perm]\n",
    "\n",
    "    label_norm = F.normalize(label,p=1,dim=-1)\n",
    "    label_norm.unsqueeze(0)\n",
    "\n",
    "    center = data.matmul(label_norm.unsqueeze(-1))\n",
    "\n",
    "    x = data\n",
    "    y = label.expand_as(x)\n",
    "    c = center.expand_as(x)\n",
    "    y_ = label.unsqueeze(-1)\n",
    "    return x,y,c\n",
    "\n",
    "# getting a hard prediction by binarizing the affinity matrix\n",
    "def getBinaryPred(model,x,beta):\n",
    "    weight = model.getWeight(beta,x)\n",
    "    weight = torch.nn.Threshold(0.8 * 1.0 / weight.shape[-1],0)(weight)\n",
    "    weight = F.normalize(weight,p=1,dim=-1)\n",
    "    predB = torch.einsum('bqi,bji -> bjq', weight, x)\n",
    "    return predB\n",
    "# helper class to record the accumulating loss\n",
    "# +=: add loss\n",
    "# print(**): print the average loss\n",
    "class loss_acc():\n",
    "    def __init__(self):\n",
    "        self.sum = 0.0\n",
    "        self.n = 0\n",
    "    def __iadd__(self,x):\n",
    "        self.sum+=x\n",
    "        self.n+=1\n",
    "        return self\n",
    "    def value(self):\n",
    "        return self.sum / self.n\n",
    "    def __str__(self):\n",
    "        return f'{self.sum/self.n:.6f}'\n",
    "    \n",
    "def test(model,testloader):\n",
    "    lossCounter = loss_acc()\n",
    "    lossCounter2 = loss_acc()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y,c in testloader:\n",
    "\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            c = c.cuda()\n",
    "            beta = x.median(dim=-1,keepdim=True)[0]\n",
    "\n",
    "            pred = model.cuda()(beta,x)\n",
    "            loss = loss_fn(pred,c[:,:,[0]])\n",
    "\n",
    "            pred_b = getBinaryPred(model,x,beta)\n",
    "            loss_b = loss_fn(pred_b,c[:,:,[0]])\n",
    "\n",
    "            lossCounter+=loss.cpu().detach().numpy()\n",
    "            lossCounter2+=loss_b.cpu().detach().numpy()\n",
    "    # print(f'{loss:.4f},{loss_b:.6f}')\n",
    "    return lossCounter, lossCounter2\n",
    "\n",
    "def test_classes(model,testloader):\n",
    "    def loss_fn(pred,gt):\n",
    "        correct = (pred == gt).all(dim=-1).sum()\n",
    "        n = pred.shape[0]\n",
    "        return correct,n\n",
    "\n",
    "    correct = 0\n",
    "    n = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y,c in testloader:\n",
    "\n",
    "            beta = x.median(dim=-1,keepdim=True)[0]\n",
    "\n",
    "\n",
    "            weight = model.getWeight(beta,x)\n",
    "            weight = torch.nn.Threshold(0.8 * 1.0 / weight.shape[-1],0)(weight)\n",
    "            pred =  (weight != 0) * 1.0\n",
    "            accuracy = loss_fn(pred,y[:,[0],:])\n",
    "            correct+=accuracy[0]\n",
    "            n+=accuracy[1]\n",
    "    # print(f'{loss:.4f},{loss_b:.6f}')\n",
    "    return correct * 1.0 / n\n",
    "\n",
    "\n",
    "def test_classes_hamming(model,testloader):\n",
    "    def loss_fn(pred,gt):\n",
    "#         print(pred.shape,gt.shape)\n",
    "#         print(\"\\n\\n\\n\",pred,\"\\n\\n\\n\",gt)\n",
    "        n = pred.shape[0]\n",
    "        correct = (pred - gt/gt.sum(2,keepdim=True)).abs().sum()\n",
    "\n",
    "        return correct,n\n",
    "\n",
    "    correct = 0\n",
    "    n = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y,c in testloader:\n",
    "\n",
    "            beta = x.median(dim=-1,keepdim=True)[0]\n",
    "\n",
    "\n",
    "            weight = model.getWeight(beta,x)\n",
    "#             weight = torch.nn.Threshold(0.8 * 1.0 / weight.shape[-1],0)(weight)\n",
    "#             pred =  (weight != 0) * 1.0\n",
    "            pred = weight\n",
    "           \n",
    "    \n",
    "            accuracy = loss_fn(pred,y[:,[0],:])\n",
    "            correct+=accuracy[0]\n",
    "            n+=accuracy[1]\n",
    "    # print(f'{loss:.4f},{loss_b:.6f}')\n",
    "    return correct * 1.0 / n\n",
    "\n",
    "class FLdata(Dataset):\n",
    "\n",
    "    def __init__(self, path_to_folder, indexes):\n",
    "        self.path_to_folder = path_to_folder\n",
    "        self.indexes = indexes\n",
    "        self.size = len(indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample = getTensorData(self.path_to_folder,self.indexes[idx])\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import math\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.main = torch.nn.Sequential(nn.Linear(in_channels, 2 * in_channels),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                nn.Linear(2 * in_channels, out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        vout=x\n",
    "        print(x.shape)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        print(x.shape)\n",
    "        attention_scores = self.main(x)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        print(vout.shape)\n",
    "        print(attention_weights.shape)\n",
    "        out = torch.einsum('bk,bjk -> bj' ,attention_weights,vout).unsqueeze(-1)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def getWeight(self, x):\n",
    "        print(x.shape)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        print(x.shape)\n",
    "        attention_scores = self.main(x).unsqueeze(1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        return attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.rand(1)<0.5).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=[\"backdoor_1(0)/gm\"]\n",
    "path_prefix=\"./AggData/dirichlet/cifar/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = ConcatDataset([FLdata(path_prefix + path_to_folder,list(range(0,30))) for path_to_folder in train_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=trainDataset[0][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.stack([trainDataset[0][0],trainDataset[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=MLP(k*10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 372, 10])\n",
      "torch.Size([2, 3720])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.getWeight(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 372, 10])\n",
      "torch.Size([2, 3720])\n",
      "torch.Size([2, 372, 10])\n",
      "torch.Size([2, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 372, 1])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=AttentionLoop(k,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 372, 1])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x.median(dim=-1,keepdims=True)[0],x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.getWeight(x.median(dim=-1,keepdims=True)[0],x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 372, 10])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0409,  0.1130,  0.0131,  ..., -0.0011,  0.0012,  0.0045],\n",
       "        [-0.0198, -0.0205, -0.1619,  ...,  0.0019, -0.0015,  0.0057]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
